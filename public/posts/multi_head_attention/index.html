<!DOCTYPE html>
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    ...
    
      <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']], 
      displayMath: [['$$', '$$'], ['\\[', '\\]']], 
      tags: "ams"  
    },
    svg: {
      fontCache: "global"
    }
  };

  
  document.addEventListener("DOMContentLoaded", function() {
    document.querySelectorAll("mjx-container[display='true']").forEach(el => {
      let wrapper = document.createElement("div");
      wrapper.className = "math-scroll";
      el.parentNode.insertBefore(wrapper, el);
      wrapper.appendChild(el);
    });
  });
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    
    ...
  </head>
<html lang="en-us"><head><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Multi-head self attention | Uzair Hussain</title>
<meta property="og:title" content="Multi-head self attention | Uzair Hussain" />
<meta name="twitter:title" content="Multi-head self attention | Uzair Hussain" />
<meta itemprop="name" content="Multi-head self attention | Uzair Hussain" />
<meta name="application-name" content="Multi-head self attention | Uzair Hussain" />
<meta property="og:site_name" content="Uzair Hussain" />

<meta name="description" content="">
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />

  <link rel="alternate" hreflang="en" href="http://192.168.0.218:1313/posts/multi_head_attention/" title="" />





    
    
    

    <meta property="og:type" content="article" />
    <meta property="og:article:published_time" content=2025-02-01T00:00:00Z />
    <meta property="article:published_time" content=2025-02-01T00:00:00Z />
    <meta property="og:url" content="http://192.168.0.218:1313/posts/multi_head_attention/" />

    
    <meta property="og:article:author" content="Uzair Hussain" />
    <meta property="article:author" content="Uzair Hussain" />
    <meta name="author" content="Uzair Hussain" />
    
    

    

    <script defer type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "Multi-head self attention",
        "author": {
        "@type": "Person",
        "name": ""
        },
        "datePublished": "2025-02-01",
        "description": "",
        "wordCount":  369 ,
        "mainEntityOfPage": "True",
        "dateModified": "2025-02-01",
        "image": {
        "@type": "imageObject",
        "url": ""
        },
        "publisher": {
        "@type": "Organization",
        "name": "Uzair Hussain"
        }
    }
    </script>


<meta name="generator" content="Hugo 0.142.0">

    
    <meta property="og:url" content="http://192.168.0.218:1313/posts/multi_head_attention/">
  <meta property="og:site_name" content="Uzair Hussain">
  <meta property="og:title" content="Multi-head self attention">
  <meta property="og:description" content="Math Here we will define and carry out a PyTorch implementation of multi-head self attention. We introduced self attention in a previous post exploring a connection with non-local means.
Let us now introduce self attention as used in transformer-like models. I will experiment with Einstein notation here to see if it makes things easier or more complicated. Below, the exponents to $\mathbb{R}$ are in the same order as the indices. For a single “head”, we have;">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-02-01T00:00:00+00:00">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Multi-head self attention">
  <meta name="twitter:description" content="Math Here we will define and carry out a PyTorch implementation of multi-head self attention. We introduced self attention in a previous post exploring a connection with non-local means.
Let us now introduce self attention as used in transformer-like models. I will experiment with Einstein notation here to see if it makes things easier or more complicated. Below, the exponents to $\mathbb{R}$ are in the same order as the indices. For a single “head”, we have;">


    

    <link rel="canonical" href="http://192.168.0.218:1313/posts/multi_head_attention/">
    <link href="/style.min.e390ba7da26222f4dc42a349955d76dbbe44e5ce2535a43de5a70633a0a9ec3c.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="http://192.168.0.218:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" crossorigin="anonymous" />


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA=="
    crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false}
      ]
    });"></script>

    
</head>

    <link rel="stylesheet" href="http://192.168.0.218:1313/css/style.min.8f53b427914b7a659ee5579181dd78bf5094b6cdc0da757f51e48ca0b92a6f98.css" integrity="sha256-j1O0J5FLemWe5VeRgd14v1CUts3A2nV/UeSMoLkqb5g=">
    <body data-theme = "light" class="notransition">

<script src="/js/theme.js"></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="http://192.168.0.218:1313/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title></title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/">
                        Home
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/resume/">
                        Resume
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link active" href="/posts/">
                        Posts
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Multi-head self attention</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2025-02-01T00:00:00&#43;00:00" itemprop="datePublished"> Feb 1, 2025 </time>
                </div>
                
            </header>
            
            <div class="page-content">
                <h3 id="math">Math</h3>
<p>Here we will define and carry out a PyTorch implementation of multi-head self attention.
We introduced self attention in a <a href="/posts/self_attention/">previous post</a> exploring a connection with non-local means.</p>
<p>Let us now introduce self attention as used in transformer-like models. I will experiment
with <a href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein notation</a> here to see if it makes things easier or more complicated. Below,
the exponents to $\mathbb{R}$ are in the same order as the indices. For a single &ldquo;head&rdquo;, we have;</p>
$$
\begin{align}
    \text{Attention}(Q, K, V)_{ai} = \text{softmax} \left( \frac{Q_{a j} K_{b}^{\ \ j}}{\sqrt{d_k}} \right) V^{b}_{\ \ \ i}
\end{align}
$$<p>where $Q_{a i}, K_{a i} \in \mathbb{R}^{N \times d_k}$, and $V_{ai} \in \mathbb{R}^{N \times d_v}$</p>
<p>and,</p>
$$
\begin{align}
    \nonumber Q_{ai} &= X_{aj} \left(W_{Q} \right)_{i}^{\ \ j}, \ \ \ \ K_{ai} = X_{aj} \left(W_{K} \right)_{i}^{\ \ j}\\ 
    & \ \ \ \text{and} \ \ \ \ V_{ai} = X_{a}^{\ j} \left(W_{V}\right)_{ij}
\end{align}
$$<p>where,</p>
<ul>
<li>\(\left(W_{Q}\right)_{ij}, \left(W_{K} \right)_{ij} \in  \mathbb{R}^{d_m \times d_k} \) and  $ \left(W_{V}\right)_{ij} \in  \mathbb{R}^{d_m \times d_v}$</li>
<li>$X_{ai} = \Xi\left( I\right) \in \mathbb{R}^{N \times d_m}$, where $I_{ai} \in \mathbb{R}^{N \times d_{\text{input}}}$ is the input signal and $\Xi$ is an embedding function.</li>
</ul>
<p>Okay so that notation looks pretty busy 😵‍💫, but things appear to be more explicit.</p>
<p>Going to multi-head attention is simple,</p>
$$
\newcommand{\Q}{Q}
\newcommand{\K}{K}
\newcommand{\V}{V}
\begin{align}
    \text{MultiHead}(\Q, \K, \V)_{ai} =\left(W_O \right)^{\mu j}_{\ \ \ \ i} \  \text{Attention}(Q,K,V)_{\mu aj}
\end{align}
$$<p>where $(W_O)_{\mu j i} \in \mathbb{R}^{h \times d_v \times d_m}$ and $h$ is the number of heads. Here the extra index, $\mu$ runs over different heads of self attention. The attention for each head is given by,</p>
$$
\begin{equation}
    \text{Attention}(Q, K, V)_{\mu a i} = \text{softmax} \left( \frac{Q_{\mu a j} K_{\mu b}^{\ \ \ \ j}}{\sqrt{d_k}} \right) V^{\ b}_{\mu \ \ i}
\end{equation}
$$<p>Note here, that although $\mu$ is repeated it is a label and a free index on the LHS.</p>
<p>I am going to abuse notation more; consider a tensor $A_{\mu a i j} \in \mathbb{R}^{n \times m \times p \times q}$ then $A_{\mu |ai,j|} \in \mathbb{R}^{n \times m p \times q}$, which is to be interpreted as a batch of matrices. These type of index fusions will help us keep track of large matrices that are GPU friendly.</p>
<h3 id="pytorch-implementation">PyTorch implementation</h3>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://github.com/uhussai7" target="_blank" rel="noopener noreferrer me"
    title="Github">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
</a>
<a href="www.linkedin.com/in/uzair-hussain-453261211" target="_blank" rel="noopener noreferrer me"
    title="Linkedin">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
</a>
<a href="mailto:ughussain@gmail.com" target="_blank" rel="noopener noreferrer me"
    title="Email">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
    <polyline points="22,6 12,13 2,6"></polyline>
</svg>
</a>
</div>
    <small class="footer_copyright">
        © 2025 Uzair Hussain.
        
    </small>
</footer>







    
    <script async src="http://192.168.0.218:1313/js/main.js" ></script>

    

</body>
</html>

